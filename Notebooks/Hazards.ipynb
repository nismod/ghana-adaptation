{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import geopandas\n",
    "import pandas\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/home/tom/OneDrive/ghana-oxford-infrastructure-adaptation-2020/'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config.json\") as fh:\n",
    "    config = json.load(fh)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/tom/OneDrive/ghana-oxford-infrastructure-adaptation-2020/incoming/hazards/Maps_Eric/flood_hazard/ID73_flood_future_eric',\n",
       " '/home/tom/OneDrive/ghana-oxford-infrastructure-adaptation-2020/incoming/hazards/Maps_Eric/flood_hazard/ID72_flood_current_eric']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flood_risk_rasters = glob.glob(os.path.join(\n",
    "    config['base_path'], 'incoming', 'hazards', 'Maps_Eric', 'flood_hazard', 'ID*'))\n",
    "\n",
    "flood_risk_rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'contains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-e3c81372ded5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"Id_future_asda\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'contains'"
     ]
    }
   ],
   "source": [
    "\"Id_future_asda\".contains(\"future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future\n",
      "\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n",
      "current\n",
      "\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for raster_path in flood_risk_rasters:\n",
    "    if \"future\" in os.path.basename(raster_path):\n",
    "        name = \"future\"\n",
    "    else:\n",
    "        name = \"current\"\n",
    "        \n",
    "    print(name)\n",
    "    tile_dir = os.path.join(\n",
    "        config['base_path'], 'data', 'hazards', name, 'tiles')\n",
    "    \n",
    "    sp = subprocess.run([\"mkdir\", \"-p\", tile_dir], capture_output=True)\n",
    "    print(sp.stdout.decode('utf-8'))\n",
    "                         \n",
    "    sp = subprocess.run(\n",
    "        [\"gdal_retile.py\", \"-ps\", \"1024\", \"1024\", \"-targetDir\", tile_dir, raster_path], \n",
    "        capture_output=True)\n",
    "    print(sp.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster_path in flood_risk_rasters:    \n",
    "    if \"future\" in os.path.basename(raster_path):\n",
    "        name = \"future\"\n",
    "    else:\n",
    "        name = \"current\"\n",
    "    \n",
    "    tile_dir = os.path.join(config['base_path'], 'data', 'hazards', name, 'tiles')\n",
    "    \n",
    "    threshold_dir = os.path.normpath(os.path.join(tile_dir, '..', 'thresholds'))\n",
    "    subprocess.run([\"mkdir\", \"-p\", threshold_dir])\n",
    "                   \n",
    "    vector_dir = os.path.normpath(os.path.join(tile_dir, '..', 'polygons'))\n",
    "    subprocess.run([\"mkdir\", \"-p\", vector_dir])\n",
    "    \n",
    "    for tile_path in glob.glob(tile_dir + '/*.tif'):\n",
    "        tile_name = os.path.basename(tile_path).replace(\".tif\", \"\")\n",
    "        \n",
    "        # thresholds = [0.0, 0.25, 0.5, 1.0, 2.0, 3.0, 5.0, 10.0, 999.0]\n",
    "        thresholds = [3,5,7]\n",
    "        \n",
    "        for min_depth, max_depth in zip(thresholds, thresholds[1:]):    \n",
    "            # Calculate mask where raster values are between thresholds\n",
    "            sp = subprocess.run([\n",
    "                \"gdal_calc.py\",\n",
    "                \"-A\", \n",
    "                f\"{tile_dir}/{tile_name}.tif\",\n",
    "                f\"--outfile={threshold_dir}/{tile_name}_{min_depth}m{max_depth}m.tif\",\n",
    "                f\"--calc=logical_and(A>={min_depth},A<{max_depth})\",\n",
    "                \"--format=GTiff\",\n",
    "                \"--type=Byte\",\n",
    "                \"--NoDataValue=0\",\n",
    "                \"--co=SPARSE_OK=YES\",\n",
    "                \"--quiet\",\n",
    "                \"--co=COMPRESS=LZW\",\n",
    "            ], check=True)\n",
    "\n",
    "            # Convert raster masks to vector polygons\n",
    "            sp = subprocess.run([\n",
    "                \"gdal_polygonize.py\",\n",
    "                f\"{threshold_dir}/{tile_name}_{min_depth}m{max_depth}m.tif\",\n",
    "                \"-q\",\n",
    "                \"-f\", \n",
    "                \"GPKG\",\n",
    "                f\"{vector_dir}/{tile_name}_{min_depth}m{max_depth}m.gpkg\"\n",
    "            ], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572ddaff91774ce2ac9e503b57205e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f70cfda69e42d68ba4946282d71250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c55e216e79d4edfb4ccc6bd2887b108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61df0eefa1449f8a0340573955440ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for raster_path in flood_risk_rasters:    \n",
    "    if \"future\" in os.path.basename(raster_path):\n",
    "        name = \"future\"\n",
    "    else:\n",
    "        name = \"current\"\n",
    "    \n",
    "    vector_dir = os.path.join(config['base_path'], 'data', 'hazards', name, 'polygons')\n",
    "    \n",
    "    for min_depth, max_depth in zip(thresholds, thresholds[1:]):    \n",
    "        match = os.path.join(vector_dir, f\"*{min_depth}m{max_depth}m.gpkg\")\n",
    "        tiles = glob.glob(match)\n",
    "        \n",
    "        dfs = []\n",
    "        for tile in tqdm(tiles):\n",
    "            df = geopandas.read_file(tile)\n",
    "            dfs.append(df)\n",
    "            \n",
    "        df = pandas.concat(dfs)\n",
    "        df.to_file(\n",
    "            os.path.join(config['base_path'], 'data', 'hazards', f\"{name}_{min_depth}-{max_depth - 1}.gpkg\"),\n",
    "            driver=\"GPKG\"\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster_path in flood_risk_rasters:    \n",
    "    if \"future\" in os.path.basename(raster_path):\n",
    "        name = \"future\"\n",
    "    else:\n",
    "        name = \"current\"\n",
    "    \n",
    "    # Clean up - remove working directories\n",
    "    working_dir = os.path.join(config['base_path'], 'data', 'hazards', name)\n",
    "    subprocess.run([\"rm\", \"-r\", working_dir])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
