{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic infrastructure intersections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table content: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Set-up - importing libraries and specifying data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import geopandas\n",
    "import pandas\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/lena/OneDrive - OnTheHub - The University of Oxford/University/Oxford/OX_2018_2019/stlucia/analysis/python/projects/nismod-int-hack/data/Hackathon/rv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in all hazards and all economic sectors as shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hazard(data_folder, hazard_id):  \n",
    "    hazard_path = os.path.join(\n",
    "        data_folder, 'data', 'hazards', '{}.shp'.format(hazard_id))\n",
    "    hazards = geopandas.read_file(hazard_path)\n",
    "    \n",
    "    if hazards.crs != {'init':'epsg:2006'}:\n",
    "        hazards = hazards.to_crs({'init':'epsg:2006'})\n",
    "    return hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sector(data_folder, sector_id):  \n",
    "    sector_path = os.path.join(\n",
    "        data_folder, 'data', 'infrastructure', '{}.shp'.format(sector_id))\n",
    "    sectors = geopandas.read_file(sector_path)\n",
    "    \n",
    "    if sectors.crs != {'init':'epsg:2006'}:\n",
    "        sectors = sectors.to_crs({'init':'epsg:2006'})\n",
    "    return sectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in merged economic infrastructures as csv and admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT FILE\n",
    "administrative_path = os.path.join(\n",
    "    data_folder, 'data','admin', 'stluc_administrative.shp')\n",
    "\n",
    "administrative = geopandas.read_file(administrative_path)\n",
    "\n",
    "#OUTPUT FILE\n",
    "econ_merged_path= os.path.join(\n",
    "    data_folder, 'data', 'infrastructure', 'econ_merged_admin.csv')\n",
    "\n",
    "intersections_econ_path = os.path.join(\n",
    "    data_folder, 'results', 'intersections_econ_merged.csv')\n",
    "\n",
    "econ_haz_cap_path = os.path.join(\n",
    "    data_folder, 'results', 'econ_hazards_capacity.csv')\n",
    "\n",
    "econ_haz_percent_exp_path = os.path.join(\n",
    "    data_folder, 'results','econ_hazards_sector%_exp.csv')\n",
    "\n",
    "econ_haz_count_path = os.path.join(\n",
    "    data_folder, 'results','econ_hazards_sector_count_exp.csv')\n",
    "\n",
    "econ_multi_haz_asset_path = os.path.join(\n",
    "    data_folder, 'results','econ_multi_hazards_assets_exp.csv')\n",
    "\n",
    "\n",
    "econ_haz_abs_path = os.path.join(\n",
    "    data_folder, 'results','econ_hazards_sector_abs_exp.csv')\n",
    "\n",
    "SDG_cap_path = os.path.join(\n",
    "    data_folder, 'results','econ_hazards_SDG_exp.csv')\n",
    "\n",
    "roads_haz_percent_exp_path = os.path.join(\n",
    "    data_folder, 'results', 'roads_hazards_%_exp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_ids = ['wastewater', 'waste','electricity', 'freight', 'airports', 'port', 'water']\n",
    "hazard_ids = ['1m_sea-level','4m_storm-surge','flashflooding','landslide_susceptibility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Adding multihazard computation\n",
    "def two_multi_hazards(row):\n",
    "    if row['4m_storm-surge'] > 0:\n",
    "        val = row['4m_storm-surge']\n",
    "    elif row['flashflooding'] > 0:\n",
    "        val = row['flashflooding']\n",
    "    else:\n",
    "        val = 0 \n",
    "    return val\n",
    "\n",
    "def three_multi_hazards(row):\n",
    "    if row['4m_storm-surge'] > 0:\n",
    "        val = row['4m_storm-surge']\n",
    "    elif row['flashflooding'] > 0:\n",
    "        val = row['flashflooding']\n",
    "    elif row['landslide_susceptibility'] > 0:\n",
    "        val = row['landslide_susceptibility']\n",
    "    else:\n",
    "        val = 0 \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only for plotting (without having to rerun analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp = pandas.read_csv(econ_haz_percent_exp_path)\n",
    "merged_intersections = pandas.read_csv(econ_haz_cap_path)\n",
    "econ_merged = pandas.read_csv(econ_merged_path)\n",
    "all_intersections = pandas.read_csv(intersections_econ_path)\n",
    "roads_exp_per = pandas.read_csv(roads_haz_percent_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged = geopandas.GeoDataFrame(econ_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(econ_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged.to_file(os.path.join(\n",
    "    data_folder, 'data', 'infrastructure','econ_merged.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all economic assets, add administrative area to each sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_sector_with_admin(sectors, administrative, sector_id):\n",
    "    \"\"\"Intersect sectors with administrative\n",
    "    \"\"\"    \n",
    "    intersections = geopandas.sjoin(sectors, administrative, how=\"inner\", op='intersects')\n",
    "    intersections = intersections[[\n",
    "        'id_left',\n",
    "        'capacity',\n",
    "        'unit',\n",
    "        'admin_name',\n",
    "        'geometry'\n",
    "    ]]\n",
    "    intersections = intersections.rename(columns={\n",
    "        'id_left': 'sector_id'\n",
    "    })\n",
    "\n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged = []\n",
    "for sector_id in sector_ids:\n",
    "    sector_df = load_sector(data_folder, sector_id)\n",
    "    sector_intersection = intersect_sector_with_admin(sector_df, administrative, sector_id)\n",
    "    econ_merged.append(sector_intersection)\n",
    "\n",
    "econ_merged = pandas.concat(econ_merged, axis=0)\n",
    "econ_merged = econ_merged.fillna(0)\n",
    "econ_merged.to_csv(econ_merged_path, index=False)\n",
    "\n",
    "econ_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersections merged economics with each hazard as separate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_hazard_with_sector(hazards, sectors, hazard_id, sector_id):\n",
    "    \"\"\"Intersect sectors with hazards\n",
    "    \"\"\"\n",
    "    ### INTERSECTIONS WITH HAZARDS        \n",
    "    intersections_sector_haz = geopandas.sjoin(sectors, hazards, how=\"inner\", op='intersects')\n",
    "\n",
    "    intersections_sector_haz[hazard_id] = 1\n",
    "\n",
    "    intersections_sector_haz['sector'] = sector_id\n",
    "\n",
    "    intersections_sector_haz = intersections_sector_haz[[\n",
    "        'sector',\n",
    "        'id_left', \n",
    "        hazard_id\n",
    "    ]]\n",
    "    \n",
    "    intersections_sector_haz = intersections_sector_haz.rename(columns={\n",
    "        'id_left': 'sector_id'\n",
    "        \n",
    "    })\n",
    "    intersections_sector_haz = intersections_sector_haz.drop_duplicates(subset='sector_id', keep='first')\n",
    "    intersections_sector_haz = intersections_sector_haz.fillna(0)\n",
    "    return intersections_sector_haz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intersections = []\n",
    "for sector_id in sector_ids:\n",
    "    sector_df = load_sector(data_folder, sector_id)\n",
    "    \n",
    "    for hazard_id in hazard_ids:\n",
    "        hazard_df = load_hazard(data_folder, hazard_id)\n",
    "        sector_intersection = intersect_hazard_with_sector(hazard_df, sector_df, hazard_id, sector_id)\n",
    "        all_intersections.append(sector_intersection)\n",
    "        \n",
    "all_intersections = pandas.concat(all_intersections, axis=0, sort=False)\n",
    "all_intersections = all_intersections.fillna(0)\n",
    "\n",
    "all_intersections.to_csv(intersections_econ_path)\n",
    "all_intersections.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point intersections for mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_ids = ['wastewater_point', 'waste_point','electricity_point', 'freight', 'airports', 'port', 'water_point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged = []\n",
    "for sector_id in sector_ids:\n",
    "    sector_df = load_sector(data_folder, sector_id)\n",
    "    econ_merged.append(sector_df)\n",
    "\n",
    "econ_merged = pandas.concat(econ_merged, axis=0)\n",
    "econ_merged = econ_merged.fillna(0)\n",
    "econ_merged = geopandas.GeoDataFrame(econ_merged)\n",
    "econ_merged.to_file(os.path.join(data_folder,'results','econ_points_merge.shp'), index=False)\n",
    "\n",
    "econ_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hazard_id in hazard_ids:\n",
    "    hazard_df = load_hazard(data_folder, hazard_id)\n",
    "    print(hazard_df)\n",
    "    sector_intersection = geopandas.sjoin(econ_merged, hazard_df, how=\"inner\", op='intersects')\n",
    "    sector_intersection = geopandas.GeoDataFrame(all_intersections)\n",
    "    sector_intersection.to_file(os.path.join(data_folder,'data','results','econ_merged_point_{}.shp'.format(hazard_id)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of all economic assets and whether they are exposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import to not rerun all analyses\n",
    "all_intersections = pandas.read_csv(intersections_econ_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intersections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intersections = all_intersections.drop(columns= ['Unnamed: 0','Unnamed: 7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections = pandas.merge(\n",
    "    econ_merged, all_intersections, how = 'outer', on=['sector','sector_id']) \n",
    "merged_intersections = merged_intersections.fillna(0)\n",
    "merged_intersections.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections.to_csv(econ_haz_count_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add capacity to each hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_ids =['1m_sea-level', '4m_storm-surge', 'flashflooding', 'landslide_susceptibility']\n",
    "for hazard_id in hazard_ids:\n",
    "    merged_intersections[hazard_id] = merged_intersections[hazard_id]* merged_intersections['capacity']\n",
    "\n",
    "merged_intersections.to_csv(econ_haz_cap_path, index=False)\n",
    "merged_intersections.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add multiple hazard calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections = pandas.read_csv(econ_haz_cap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list =['1m_sea-level', '4m_storm-surge', 'flashflooding', 'landslide_susceptibility', 'capacity']\n",
    "#ensure all values are numeric\n",
    "merged_intersections[list] = merged_intersections[list].apply(pandas.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge information for each asset - grouping by \n",
    "# -> If sector_id is the same, add the columns for each of the hazards\n",
    "merged_intersections = merged_intersections.groupby([\n",
    "    'sector', 'sector_id', 'admin_name', 'unit', 'geometry'\n",
    "    ])[\n",
    "    list\n",
    "    ].sum()\n",
    "merged_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new hazard with combined hazard \n",
    "merged_intersections['storm_flash'] = merged_intersections.apply(two_multi_hazards, axis=1)\n",
    "merged_intersections['storm_flash_landslide']= merged_intersections.apply(three_multi_hazards, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections.to_csv(econ_multi_haz_asset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Calculate absolute exposed capacity per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list =['1m_sea-level', '4m_storm-surge', 'flashflooding', 'landslide_susceptibility','storm_flash','storm_flash_landslide', 'capacity']\n",
    "sector_exp = merged_intersections.groupby(['sector', 'unit'])[list].sum().reset_index()\n",
    "sector_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp.to_csv(econ_haz_abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate % of exposed capacity per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per = sector_exp.copy()\n",
    "hazard_ids =['1m_sea-level', '4m_storm-surge', 'flashflooding', 'landslide_susceptibility', 'storm_flash', 'storm_flash_landslide']\n",
    "for hazard_id in hazard_ids:\n",
    "    sector_exp_per[hazard_id] = round((sector_exp_per[hazard_id] / sector_exp_per['capacity']*100))\n",
    "    \n",
    "sector_exp_per = sector_exp_per.fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per.index = sector_exp_per.sector\n",
    "#sector_exp_per = sector_exp_per.drop('sector', axis=1)\n",
    "\n",
    "sector_exp_per.to_csv(econ_haz_percent_exp_path)\n",
    "sector_exp_per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD ROADS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add roads as sector \n",
    "sector_exp_per = pandas.read_csv(econ_haz_percent_exp_path, index_col = 'sector')\n",
    "roads_exp_per = pandas.read_csv(roads_haz_percent_exp_path, index_col = 'sector')\n",
    "sector_exp_per = pandas.concat([sector_exp_per,roads_exp_per], axis=0,  sort=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_exp_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per= sector_exp_per.replace('cargo','freight')\n",
    "sector_exp_per "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per = sector_exp_per.drop(columns='sector.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per = sector_exp_per.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_exp_per.to_csv(econ_haz_percent_exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SDG part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDG_direct_path = os.path.join(\n",
    "        data_folder, 'data', 'SDG', 'SDG_influence_eco_soc_ecosystem_direct.csv')\n",
    "\n",
    "SDG_direct = pandas.read_csv(SDG_direct_path, index_col ='Target')\n",
    "SDG_direct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDG_direct = SDG_direct.sum()\n",
    "SDG_direct_sec = pandas.DataFrame(SDG_direct)\n",
    "SDG_direct_sec = SDG_direct_sec.rename(columns ={\n",
    "    0:'SDG_direct'\n",
    "})\n",
    "SDG_direct_sec.index.name='sector'\n",
    "SDG_direct_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDG_exp = sector_exp_per.merge(SDG_direct_sec, on ='sector' )\n",
    "SDG_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hazard_id in hazard_ids:\n",
    "    SDG_exp[hazard_id] = SDG_exp[hazard_id]* SDG_exp['SDG_direct']\n",
    "\n",
    "SDG_exp.to_csv(SDG_cap_path, index=True)\n",
    "SDG_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT ECONOMIC ASSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sector_exp = pandas.read_csv(econ_haz_percent_exp_path)\n",
    "plot_econ_path = os.path.join(\n",
    "        data_folder, 'figures', 'fig_economic_inf_stagged_hazards.svg')\n",
    "\n",
    "plot_econ_SDG_path = os.path.join(\n",
    "        data_folder, 'figures', 'fig_economic_SDG_hazards.svg')\n",
    "\n",
    "SDG_cap_path = os.path.join(\n",
    "    data_folder, 'results','econ_hazards_SDG_exp.csv')\n",
    "\n",
    "SDG_direct_path = os.path.join(\n",
    "        data_folder, 'data', 'SDG', 'SDG_influence_eco_soc_ecosystem_direct.csv')\n",
    "\n",
    "SDG_direct = pandas.read_csv(SDG_direct_path)\n",
    "\n",
    "SDG_exp = pandas.read_csv(SDG_cap_path)\n",
    "\n",
    "sector_exp = pandas.read_csv(econ_haz_percent_exp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR SDG color=['red','darkred','yellow','lightcoral','darkgoldenrod','lightblue','blue']\n",
    "def plot_to_ax(ax, title, hazard):\n",
    "    ax = hazard.plot(ax=ax,kind='bar',use_index=True, legend = False,color=['red','green','yellow','orange','grey','lightblue','blue', 'black'], alpha=0.4)#x=sector_exp['sector']\n",
    "    #\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_title(title,fontweight = 'bold')\n",
    "    ax.set_ylabel('Percentage of total capacity exposed')\n",
    "    #ax.set_ylim(0, 100)\n",
    "    ax.tick_params(axis='x', rotation = 90)\n",
    "    ax.set_xlabel('economic infrastructures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2,sharex=True,sharey=True,figsize=(10,10), tight_layout = True)\n",
    "\n",
    "plot_to_ax(axes[0,0], 'Sea-level rise (1 metre)', sector_exp['1m_sea-level'])\n",
    "plot_to_ax(axes[0,1], 'Storm surge (4 metre)', sector_exp['4m_storm-surge'])\n",
    "plot_to_ax(axes[1,0], 'Flashflooding (low, mod, high & very high susceptibility)', sector_exp['flashflooding'])\n",
    "plot_to_ax(axes[1,1], 'Landslides (high susceptibility)', sector_exp['landslide_susceptibility'])\n",
    "plt.savefig(plot_econ_path)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2,sharex=True,sharey=True,figsize=(10,10), tight_layout = True)\n",
    "\n",
    "plot_to_ax(ax[0,0], 'Sea-level rise (1 metre)', sector_exp['1m_sea-level'])\n",
    "plot_to_ax(ax[0,1], 'Storm surge (4 metre)', sector_exp['4m_storm-surge'])\n",
    "plot_to_ax(ax[1,0], 'Flashflooding (low, mod, high & very high susceptibility)', sector_exp['flashflooding'])\n",
    "plot_to_ax(ax[1,1], 'Landslides (high susceptibility)', sector_exp['landslide_susceptibility'])\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "plot_to_ax2(ax2[0,0], 'Sea-level rise (1 metre)', SDG_exp['1m_sea-level'])\n",
    "plot_to_ax2(ax2[0,1], 'Storm surge (4 metre)', SDG_exp['4m_storm-surge'])\n",
    "plot_to_ax2(ax2[1,0], 'Flashflooding (low, mod, high & very high susceptibility)', SDG_exp['flashflooding'])\n",
    "plot_to_ax2(ax2[1,1], 'Landslides (high susceptibility)', SDG_exp['landslide_susceptibility'])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(plot_econ_SDG_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BY ADMIN CALCULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE ADMIN WITH ALL ASSETS, CUMULATIVE RISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_econ_cum_exp_path = os.path.join(\n",
    "    data_folder, 'results', 'admin_econ_cum_geo.csv')\n",
    "\n",
    "admin_econ_cum_exp_disag_path = os.path.join(\n",
    "    data_folder, 'results', 'admin_econ_cum_disag.csv')\n",
    "\n",
    "admin_econ_exp_path = os.path.join(\n",
    "    data_folder, 'results','admin_econ_sector_exp.csv')\n",
    "\n",
    "merged_intersections = pandas.read_csv(econ_multi_haz_asset_path)\n",
    "admin_econ_mean_exp_path= os.path.join(\n",
    "    data_folder, 'results', 'admin_econ_mean.csv')\n",
    "\n",
    "hazard_ids = ['1m_sea-level','4m_storm-surge','flashflooding','landslide_susceptibility', 'storm_flash', 'storm_flash_landslide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_intersections= merged_intersections.replace('cargo','freight')\n",
    "merged_intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_admin = merged_intersections.groupby(['admin_name','sector', 'unit']).sum().reset_index()\n",
    "econ_admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting overview of percentage of sector exposed with admin info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_admin_stagged = econ_admin.copy()\n",
    "for hazard_id in hazard_ids:\n",
    "    econ_admin_stagged[hazard_id] = round((econ_admin_stagged[hazard_id] / econ_admin_stagged.capacity)*100)\n",
    "econ_admin_stagged = econ_admin_stagged.drop(columns={'unit', 'sector_id', 'capacity'})\n",
    "econ_admin_stagged = econ_admin_stagged.set_index('admin_name')\n",
    "econ_admin_stagged.to_csv(admin_econ_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_admin_stagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing sector exposure for all different administrative areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_admin(sector_id, administrative):\n",
    "    sector = econ_admin[econ_admin['sector'] == sector_id]\n",
    "    admin_econ_sector =  pandas.merge(administrative, sector, on ='admin_name', how='left')\n",
    "    admin_econ_sector['sector'] = sector_id\n",
    "    \n",
    "    return admin_econ_sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export each admin and sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector_id in sector_ids:\n",
    "    admin_econ = merge_admin(sector_id, administrative)\n",
    "    admin_econ = geopandas.GeoDataFrame(admin_econ)\n",
    "    admin_econ.crs = {'init': 'epsg:2006'}\n",
    "    admin_econ = admin_econ.reset_index()\n",
    "    for hazard_id in hazard_ids:\n",
    "        admin_econ[hazard_id] = round((admin_econ[hazard_id] / admin_econ['capacity'])*100)\n",
    "    admin_econ = admin_econ.rename(columns={\n",
    "        '1m_sea-level': 'sea-lev_1m',\n",
    "        '4m_storm-surge': 'storm_s_4m',\n",
    "        'flashflooding': 'flashflood', \n",
    "        'landslide_susceptibility':'landslide'\n",
    "    })\n",
    "    admin_econ = admin_econ.drop(columns=[ 'sector_id', 'index', 'id'])\n",
    "    admin_econ = admin_econ.fillna(0)\n",
    "    print(admin_econ)\n",
    "    admin_econ_path = os.path.join(\n",
    "        data_folder, 'results','admin_{}.shp'.format(sector_id))\n",
    "    admin_econ.to_file(admin_econ_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [old_v072019]Combine administrative areas for each asset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_econ_sector = []\n",
    "for sector_id in sector_ids:\n",
    "    admin_econ = merge_admin(sector_id, administrative)\n",
    "    admin_econ_sector.append(admin_econ)\n",
    "admin_econ_sector = pandas.concat(admin_econ_sector, axis=0)\n",
    "admin_econ_sector_ind = admin_econ_sector.set_index('admin_name')\n",
    "admin_econ_sector_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hazard_id in hazard_ids:\n",
    "    admin_econ_sector[hazard_id] = round((admin_econ_sector[hazard_id] / admin_econ_sector.capacity)*100)\n",
    "    #sector_exp = rename_hazard_ids(hazard_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sector_exp.index = sector_exp.sector\n",
    "admin_econ_sector = admin_econ_sector.drop(['geometry','unit','sector_id','capacity','id'], axis=1)\n",
    "admin_econ_sector_ind = admin_econ_sector.set_index('admin_name')\n",
    "admin_econ_sector_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_econ_sector_ind.to_csv(admin_econ_cum_exp_disag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_econ_cum_exp = admin_econ_sector.groupby(['admin_name']).sum().reset_index()\n",
    "\n",
    "admin_econ_cum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_econ_cum_exp.to_csv(admin_econ_cum_exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
